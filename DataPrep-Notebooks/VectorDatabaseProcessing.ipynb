{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2b5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-mini-lm embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No chunks file found. Starting full process from scratch.\n",
      "--- Step 1: Processing PDFs from '/Users/jimharrington/Desktop/ANEET/Physics/Physics' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:29<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 624 document(s).\n",
      "Cleaning up document metadata...\n",
      "Metadata cleaned. Example source: 'Physics_11th_NCRT_BOOK_Unit1Chapter_7'\n",
      "Split documents into 1399 chunks.\n",
      "Saving processed chunks to './processed_physics_chunks.json'...\n",
      "--- Step 1 Complete: Chunks saved successfully. ---\n",
      "Found and removing existing DB directory: './chroma_vector_db_physics'\n",
      "Creating new empty directory for DB: './chroma_vector_db_physics'\n",
      "\n",
      "--- Step 2: Creating Vector DB from './processed_physics_chunks.json' ---\n",
      "Loaded 1399 chunks from file.\n",
      "Initializing embedding model: 'all-MiniLM-L6-v2'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized.\n",
      "Creating and persisting vector store at './chroma_vector_db_physics'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2 Complete: Vector DB Creation Complete! ---\n",
      "Vector store saved to: ./chroma_vector_db_physics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil # Import the shutil library for directory operations\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- CONSTANTS ---\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# ... (The process_and_save_chunks and create_vectordb_from_chunks functions remain unchanged) ...\n",
    "def process_and_save_chunks(pdf_folder: str, chunks_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Step 1: Load PDFs, clean up metadata, split them into chunks, and save them to a JSON file.\n",
    "    Returns True on success, False on failure.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(pdf_folder):\n",
    "        print(f\"Error: The folder '{pdf_folder}' does not exist.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"--- Step 1: Processing PDFs from '{pdf_folder}' ---\")\n",
    "\n",
    "    # Load documents\n",
    "    loader = DirectoryLoader(\n",
    "        pdf_folder,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True,\n",
    "        use_multithreading=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    if not documents:\n",
    "        print(\"No PDF documents found. Exiting.\")\n",
    "        return False\n",
    "    print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "    # Clean up the 'source' metadata to be the chapter name (filename without extension)\n",
    "    print(\"Cleaning up document metadata...\")\n",
    "    for doc in documents:\n",
    "        source_path = doc.metadata.get('source', '')\n",
    "        filename = os.path.basename(source_path)\n",
    "        chapter_name, _ = os.path.splitext(filename)\n",
    "        doc.metadata['source'] = chapter_name\n",
    "    print(f\"Metadata cleaned. Example source: '{documents[0].metadata['source']}'\")\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split documents into {len(splits)} chunks.\")\n",
    "\n",
    "    # Save chunks to a JSON file for later use\n",
    "    print(f\"Saving processed chunks to '{chunks_path}'...\")\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        json_data = [\n",
    "            {'page_content': doc.page_content, 'metadata': doc.metadata}\n",
    "            for doc in splits\n",
    "        ]\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    print(\"--- Step 1 Complete: Chunks saved successfully. ---\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def create_vectordb_from_chunks(db_path: str, chunks_path: str):\n",
    "    \"\"\"\n",
    "    Step 2: Load the processed chunks from the JSON file and create the\n",
    "    persistent Chroma vector database.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(chunks_path):\n",
    "        print(f\"Error: Chunks file not found at '{chunks_path}'. Please run the processing step first.\")\n",
    "        return\n",
    "        \n",
    "    # Clean up old database directory if it exists to prevent errors\n",
    "    if os.path.exists(db_path):\n",
    "        print(f\"Found and removing existing DB directory: '{db_path}'\")\n",
    "        shutil.rmtree(db_path)\n",
    "\n",
    "    # <<< FIX >>> Create the new, empty directory for the database.\n",
    "    # ChromaDB expects the directory to exist before it can write to it.\n",
    "    print(f\"Creating new empty directory for DB: '{db_path}'\")\n",
    "    os.makedirs(db_path)\n",
    "\n",
    "    print(f\"\\n--- Step 2: Creating Vector DB from '{chunks_path}' ---\")\n",
    "\n",
    "    # Load the processed chunks from the JSON file\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Re-create Document objects from the loaded data\n",
    "    documents_from_json = [\n",
    "        Document(page_content=item['page_content'], metadata=item['metadata'])\n",
    "        for item in json_data\n",
    "    ]\n",
    "    print(f\"Loaded {len(documents_from_json)} chunks from file.\")\n",
    "\n",
    "    # Initialize the Sentence Transformer embedding model\n",
    "    print(f\"Initializing embedding model: '{EMBEDDING_MODEL_NAME}'...\")\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    print(\"Embedding model initialized.\")\n",
    "\n",
    "    # Create and persist the Chroma vector store\n",
    "    print(f\"Creating and persisting vector store at '{db_path}'...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents_from_json,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    print(\"--- Step 2 Complete: Vector DB Creation Complete! ---\")\n",
    "    print(f\"Vector store saved to: {db_path}\")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# === SCRIPT EXECUTION =============================================\n",
    "# ==================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURE YOUR PATHS HERE ---\n",
    "    pdf_source_folder = \"/Users/jimharrington/Desktop/ANEET/Quiz/Dataprep/Processed_papers\"\n",
    "    chunks_output_path = \"/Users/jimharrington/Desktop/ANEET/solved_question_papers.json\"\n",
    "    db_output_path = \"/Users/jimharrington/Desktop/ANEET/chroma_vector_db_solved_question_papers_normic\"\n",
    "\n",
    "    # --- MAIN EXECUTION LOGIC ---\n",
    "    if os.path.exists(chunks_output_path):\n",
    "        print(f\"Found existing chunks file: '{chunks_output_path}'\")\n",
    "        print(\"Skipping PDF processing. Proceeding directly to embedding.\")\n",
    "        create_vectordb_from_chunks(db_path=db_output_path, chunks_path=chunks_output_path)\n",
    "    else:\n",
    "        print(f\"No chunks file found. Starting full process from scratch.\")\n",
    "        if process_and_save_chunks(pdf_folder=pdf_source_folder, chunks_path=chunks_output_path):\n",
    "            create_vectordb_from_chunks(db_path=db_output_path, chunks_path=chunks_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb3fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database from ./chroma_vector_db_physics...\n",
      "Performing a test search...\n",
      "\n",
      "--- Search Results ---\n",
      "Source Chapter: Physics_11th_NCRT_BOOK_Unit1Chapter_7\n",
      "Page: 6\n",
      "Content: GRAVITATION 133\n",
      "hence\n",
      "E\n",
      "3\n",
      "E\n",
      "G m M rR= (7.10)\n",
      "If the mass m is situated on the surface of\n",
      "earth, then  r = RE and the gravitational force on\n",
      "it is, from Eq. (7.10)\n",
      "2\n",
      "E\n",
      "E\n",
      "M mF G R= (7.11)\n",
      "The accelerati...\n",
      "--------------------\n",
      "Source Chapter: Physics_11th_NCRT_BOOK_Unit1Chapter_7\n",
      "Page: 4\n",
      "Content: GRAVITATION 131\n",
      "cases, a  simple law results when you do that :\n",
      "(1) The force of attraction between a hollow\n",
      "spherical shell of uniform density and a\n",
      "point mass situated outside is just as if\n",
      "the enti...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "db_path_to_query = \"./chroma_vector_db_physics\"\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the persisted database\n",
    "print(f\"Loading database from {db_path_to_query}...\")\n",
    "db = Chroma(persist_directory=db_path_to_query, embedding_function=embedding_function)\n",
    "\n",
    "# Perform a similarity search\n",
    "print(\"Performing a test search...\")\n",
    "# Replace \"cell biology\" with a term relevant to your documents\n",
    "results = db.similarity_search(\"what is the value of gravity?\", k=2)\n",
    "\n",
    "# Print the results and inspect the metadata\n",
    "print(\"\\n--- Search Results ---\")\n",
    "for doc in results:\n",
    "    print(f\"Source Chapter: {doc.metadata.get('source')}\") # Should be \"kebo1ps\" etc.\n",
    "    print(f\"Page: {doc.metadata.get('page')}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\") # Print snippet of the content\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93debaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6b45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No chunks file found. Starting full process from scratch.\n",
      "--- Step 1: Processing PDFs from '/Users/jimharrington/Desktop/ANEET/NCERT Books Raw Data/MentorGuide' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.49it/s]\n",
      "/var/folders/rc/216ly2m96l53n9ybxf3h14b00000gn/T/ipykernel_1886/4112371840.py:102: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 document(s).\n",
      "Cleaning up document metadata...\n",
      "Metadata cleaned. Example source: 'ANEETA_NEET_Assistance_Description'\n",
      "Split documents into 48 chunks.\n",
      "Saving processed chunks to '/Users/jimharrington/Desktop/ANEET/Processed Data/mentor_data.json'...\n",
      "--- Step 1 Complete: Chunks saved successfully. ---\n",
      "Creating new empty directory for DB: '/Users/jimharrington/Desktop/ANEET/VectorDB/nomicLM-Embed-VectorDB/chroma_vector_db_mentor_nomic'\n",
      "\n",
      "--- Step 2: Creating Vector DB from '/Users/jimharrington/Desktop/ANEET/Processed Data/mentor_data.json' ---\n",
      "Loaded 48 chunks from file.\n",
      "Initializing Ollama embedding model: 'nomic-embed-text'...\n",
      "Ollama embedding model initialized.\n",
      "Creating and persisting vector store at '/Users/jimharrington/Desktop/ANEET/VectorDB/nomicLM-Embed-VectorDB/chroma_vector_db_mentor_nomic'...\n",
      "--- Step 2 Complete: Vector DB Creation Complete! ---\n",
      "Vector store saved to: /Users/jimharrington/Desktop/ANEET/VectorDB/nomicLM-Embed-VectorDB/chroma_vector_db_mentor_nomic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# <<< FIX 1 >>> Import OllamaEmbeddings instead of SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- CONSTANTS ---\n",
    "# <<< FIX 2 >>> Define the name of the model as it's known in Ollama\n",
    "OLLAMA_EMBEDDING_MODEL = \"nomic-embed-text\" \n",
    "\n",
    "def process_and_save_chunks(pdf_folder: str, chunks_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Step 1: Load PDFs, clean up metadata, split them into chunks, and save them to a JSON file.\n",
    "    Returns True on success, False on failure.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(pdf_folder):\n",
    "        print(f\"Error: The folder '{pdf_folder}' does not exist.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"--- Step 1: Processing PDFs from '{pdf_folder}' ---\")\n",
    "\n",
    "    # Load documents\n",
    "    loader = DirectoryLoader(\n",
    "        pdf_folder,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True,\n",
    "        use_multithreading=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    if not documents:\n",
    "        print(\"No PDF documents found. Exiting.\")\n",
    "        return False\n",
    "    print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "    # Clean up the 'source' metadata to be the chapter name (filename without extension)\n",
    "    print(\"Cleaning up document metadata...\")\n",
    "    for doc in documents:\n",
    "        source_path = doc.metadata.get('source', '')\n",
    "        filename = os.path.basename(source_path)\n",
    "        chapter_name, _ = os.path.splitext(filename)\n",
    "        doc.metadata['source'] = chapter_name\n",
    "    print(f\"Metadata cleaned. Example source: '{documents[0].metadata['source']}'\")\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split documents into {len(splits)} chunks.\")\n",
    "\n",
    "    # Save chunks to a JSON file for later use\n",
    "    print(f\"Saving processed chunks to '{chunks_path}'...\")\n",
    "    with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "        json_data = [\n",
    "            {'page_content': doc.page_content, 'metadata': doc.metadata}\n",
    "            for doc in splits\n",
    "        ]\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    print(\"--- Step 1 Complete: Chunks saved successfully. ---\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def create_vectordb_from_chunks(db_path: str, chunks_path: str):\n",
    "    \"\"\"\n",
    "    Step 2: Load the processed chunks from the JSON file and create the\n",
    "    persistent Chroma vector database using Ollama.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(chunks_path):\n",
    "        print(f\"Error: Chunks file not found at '{chunks_path}'. Please run the processing step first.\")\n",
    "        return\n",
    "        \n",
    "    # Clean up old database directory if it exists\n",
    "    if os.path.exists(db_path):\n",
    "        print(f\"Found and removing existing DB directory: '{db_path}'\")\n",
    "        shutil.rmtree(db_path)\n",
    "\n",
    "    # Create the new, empty directory for the database.\n",
    "    print(f\"Creating new empty directory for DB: '{db_path}'\")\n",
    "    os.makedirs(db_path)\n",
    "\n",
    "    print(f\"\\n--- Step 2: Creating Vector DB from '{chunks_path}' ---\")\n",
    "\n",
    "    # Load the processed chunks from the JSON file\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Re-create Document objects from the loaded data\n",
    "    documents_from_json = [\n",
    "        Document(page_content=item['page_content'], metadata=item['metadata'])\n",
    "        for item in json_data\n",
    "    ]\n",
    "    print(f\"Loaded {len(documents_from_json)} chunks from file.\")\n",
    "\n",
    "    # <<< FIX 3 >>> Initialize the Ollama embedding model\n",
    "    # Make sure your Ollama application is running and has the specified model.\n",
    "    print(f\"Initializing Ollama embedding model: '{OLLAMA_EMBEDDING_MODEL}'...\")\n",
    "    embeddings = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL)\n",
    "    print(\"Ollama embedding model initialized.\")\n",
    "\n",
    "    # Create and persist the Chroma vector store\n",
    "    print(f\"Creating and persisting vector store at '{db_path}'...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents_from_json,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    print(\"--- Step 2 Complete: Vector DB Creation Complete! ---\")\n",
    "    print(f\"Vector store saved to: {db_path}\")\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# === SCRIPT EXECUTION =============================================\n",
    "# ==================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURE YOUR PATHS HERE ---\n",
    "    pdf_source_folder = \"/Users/jimharrington/Desktop/ANEET/NCERT Books Raw Data/MentorGuide\"\n",
    "    chunks_output_path = \"/Users/jimharrington/Desktop/ANEET/Processed Data/mentor_data.json\"\n",
    "    db_output_path = \"/Users/jimharrington/Desktop/ANEET/VectorDB/nomicLM-Embed-VectorDB/chroma_vector_db_mentor_nomic\"\n",
    "\n",
    "    # --- MAIN EXECUTION LOGIC ---\n",
    "    if os.path.exists(chunks_output_path):\n",
    "        print(f\"Found existing chunks file: '{chunks_output_path}'\")\n",
    "        print(\"Skipping PDF processing. Proceeding directly to embedding.\")\n",
    "        create_vectordb_from_chunks(db_path=db_output_path, chunks_path=chunks_output_path)\n",
    "    else:\n",
    "        print(f\"No chunks file found. Starting full process from scratch.\")\n",
    "        if process_and_save_chunks(pdf_folder=pdf_source_folder, chunks_path=chunks_output_path):\n",
    "            create_vectordb_from_chunks(db_path=db_output_path, chunks_path=chunks_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2329b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama embeddings with model: 'nomic-embed-text'...\n",
      "Embedding function initialized.\n",
      "Loading database from: '/Users/jimharrington/Desktop/ANEET/chroma_vector_db_mentor_nomic'...\n",
      "Database loaded successfully.\n",
      "\n",
      "Performing a similarity search for: 'top medical colleges?'\n",
      "\n",
      "--- Search Results ---\n",
      "--- Result 1 ---\n",
      "Source Chapter: mentor_knowledge\n",
      "Page Number: 18\n",
      "Content: which\n",
      " \n",
      "you\n",
      " \n",
      "can\n",
      " \n",
      "follow\n",
      " \n",
      "to\n",
      " \n",
      "enhance\n",
      " \n",
      "your\n",
      " \n",
      "readiness\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "exam.\n",
      "  ●  Familiarize  with  the  syllabus  ●  Invest  In  Good  Quality  Study  Materials  ●  Create  a  Certain  Strategy  and  Follow   ●  Finish  the  Most  Important  Topics  First  ●  Create  and  Stick  To  a  Timetable  ●  Make  your  Own  Notes  ●  Take  Breaks  Re...\n",
      "-------------------------\n",
      "--- Result 2 ---\n",
      "Source Chapter: mentor_knowledge\n",
      "Page Number: 18\n",
      "Content: Rank  College  City  State  \n",
      "1  All  India  Institute  of  Medical  Sciences,  Delhi  New  Delhi  Delhi  \n",
      "2  \n",
      "Post  Graduate  Institute  of  Medical  Education  and  Research  \n",
      "Chandigarh  Chandigarh  \n",
      "3  Christian  Medical  College  Vellore  Tamil  Nadu  \n",
      "4  \n",
      "National  Institute  of  Mental  Health  &  Neuro  Sciences  \n",
      "Bangalore  Karnataka...\n",
      "-------------------------\n",
      "--- Result 3 ---\n",
      "Source Chapter: mentor_knowledge\n",
      "Page Number: 21\n",
      "Content: 43  Sawai  Man  Singh  Medical  College  Jaipur  Rajasthan  \n",
      "44  Medical  College  Kolkata  West  Bengal  \n",
      "45  Gujarat  Cancer  &  Research  Institute  Ahmadabad  Gujarat  \n",
      "46  M.  S.  Ramaiah  Medical  College  Bengaluru  Karnataka  \n",
      "47  \n",
      "Mahatma  Gandhi  Medical  College  and  Research  Institute  \n",
      "Puducherry  Pondicherry  \n",
      "48  Osmania  Medical  ...\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/216ly2m96l53n9ybxf3h14b00000gn/T/ipykernel_1176/664079198.py:35: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# <<< FIX 1 >>> Import the OllamaEmbeddings class\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# --- CONFIGURE YOUR QUERY HERE ---\n",
    "\n",
    "# <<< FIX 2 >>> Point to the database you created with Ollama\n",
    "db_path_to_query = \"/Users/jimharrington/Desktop/ANEET/chroma_vector_db_mentor_nomic\" \n",
    "\n",
    "# <<< FIX 3 >>> Use the exact same model name as in the creation script\n",
    "# This is the name Ollama uses, not the Hugging Face identifier.\n",
    "OLLAMA_MODEL_NAME = \"nomic-embed-text\" \n",
    "\n",
    "# Your query\n",
    "query_text = \"top medical colleges?\"\n",
    "# Number of results to return\n",
    "k_results = 3\n",
    "\n",
    "# --- SCRIPT EXECUTION ---\n",
    "\n",
    "# Check if the database directory exists\n",
    "if not os.path.exists(db_path_to_query):\n",
    "    print(f\"Error: Database not found at '{db_path_to_query}'\")\n",
    "    print(\"Please make sure you have run the creation script first and the path is correct.\")\n",
    "else:\n",
    "    # <<< FIX 4 >>> Initialize the Ollama embedding function, NOT SentenceTransformerEmbeddings\n",
    "    # This ensures your query is converted to a vector in the same way as the documents were.\n",
    "    print(f\"Initializing Ollama embeddings with model: '{OLLAMA_MODEL_NAME}'...\")\n",
    "    embedding_function = OllamaEmbeddings(model=OLLAMA_MODEL_NAME)\n",
    "    print(\"Embedding function initialized.\")\n",
    "\n",
    "    # Load the persisted database\n",
    "    print(f\"Loading database from: '{db_path_to_query}'...\")\n",
    "    db = Chroma(\n",
    "        persist_directory=db_path_to_query, \n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(\"Database loaded successfully.\")\n",
    "\n",
    "    # Perform a similarity search\n",
    "    print(f\"\\nPerforming a similarity search for: '{query_text}'\")\n",
    "    results = db.similarity_search(query_text, k=k_results)\n",
    "\n",
    "    # Print the results and inspect the metadata\n",
    "    print(\"\\n--- Search Results ---\")\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "    else:\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"--- Result {i+1} ---\")\n",
    "            print(f\"Source Chapter: {doc.metadata.get('source', 'N/A')}\")\n",
    "            print(f\"Page Number: {doc.metadata.get('page', 'N/A')}\")\n",
    "            print(f\"Content: {doc.page_content[:350]}...\") # Print a slightly longer snippet\n",
    "            print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c28e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2390bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
