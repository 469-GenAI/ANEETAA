{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a008887",
   "metadata": {},
   "source": [
    "# ANEETA Evaluation: Local RAG and Model Comparisons\n",
    "This notebook provides a reproducible baseline evaluation of the ANEETA Doubt Solver using local models via Ollama and the existing ANEETA vector stores. It also verifies repository setup and supports simple model comparisons for group reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d87055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: D:\\Git Projects\\SMU\\ANEETAA\n",
      "ANEETAA repo at: D:\\Git Projects\\SMU\\ANEETAA\n",
      "âœ“ Found: src\n",
      "âœ“ Found: Processed Data\n",
      "âœ“ Found: Raw Data\n",
      "Repo verified.\n"
     ]
    }
   ],
   "source": [
    "# 1) Verify Local Repos and Paths\n",
    "import os, sys, subprocess, shutil\n",
    "from pathlib import Path\n",
    "ROOT = Path.cwd().resolve()\n",
    "# This notebook lives INSIDE the ANEETAA repo\n",
    "aneeta_dir = ROOT  # current directory IS the ANEETAA repo\n",
    "print(\"Current working dir:\", ROOT)\n",
    "print(\"ANEETAA repo at:\", aneeta_dir)\n",
    "\n",
    "# Verify we have the key ANEETAA directories\n",
    "required_dirs = [\"src\", \"Processed Data\", \"Raw Data\"]\n",
    "for d in required_dirs:\n",
    "    if (aneeta_dir / d).exists():\n",
    "        print(f\"âœ“ Found: {d}\")\n",
    "    else:\n",
    "        print(f\"âœ— Missing: {d}\")\n",
    "\n",
    "print(\"Repo verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20bb8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n",
      "Executable: c:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "Platform: Windows-10-10.0.26200-SP0\n",
      "Suggested venv path: D:\\Git Projects\\SMU\\ANEETAA\\.venv\n",
      "If you need one, run in PowerShell:\n",
      "python -m venv .venv; .\\.venv\\Scripts\\Activate.ps1; python -m pip install -U pip\n",
      "Torch not installed or CUDA not available: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "# 2) Create and Validate Python Environment\n",
    "import platform, sys, subprocess, os\n",
    "from pathlib import Path\n",
    "print(\"Python:\", sys.version)\n",
    "assert sys.version_info >= (3,10), \"Python >= 3.10 required\"\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optionally create venv instructions (not auto-creating to avoid permission issues)\n",
    "venv_dir = ROOT / \".venv\"\n",
    "print(\"Suggested venv path:\", venv_dir)\n",
    "print(\"If you need one, run in PowerShell:\")\n",
    "print(\"python -m venv .venv; .\\\\.venv\\\\Scripts\\\\Activate.ps1; python -m pip install -U pip\")\n",
    "\n",
    "# CUDA check (optional)\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed or CUDA not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a685551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing core packages (binary wheels preferred)...\n",
      "Core imports OK (lean setup)\n",
      "Core imports OK (lean setup)\n"
     ]
    }
   ],
   "source": [
    "# 3) Install and Import Dependencies (lean and fast on Windows)\n",
    "import sys, subprocess, os\n",
    "\n",
    "# Speed: upgrade pip tooling first\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "\n",
    "# Minimal core deps; avoid heavy extras here\n",
    "# Use binary wheels where possible to speed up installs\n",
    "packages = [\n",
    "    \"typing_extensions>=4.12.2\",\n",
    "    \"pydantic<2.0.0\",        # compatible with chromadb 0.4.x\n",
    "    \"numpy==1.26.4\",         # stable wheels\n",
    "    \"pandas==2.2.2\",         # pin to a wheel-friendly version\n",
    "    \"requests\",\n",
    "    \"tqdm>=4.65.0\",\n",
    "    \"jsonlines\",\n",
    "    \"chromadb==0.4.24\",      # no ONNX default embedder\n",
    "    # Minimal ANEETA runtime deps (pure-Python)\n",
    "    \"langchain\", \"langchain-community\", \"langchain-ollama\", \"langgraph\"\n",
    "]\n",
    "\n",
    "print(\"Installing core packages (binary wheels preferred)...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--only-binary\", \":all:\", *packages], check=False)\n",
    "\n",
    "# Imports (defer MLflow to services cell and make it optional)\n",
    "import chromadb, pandas as pd, numpy as np, requests  # noqa: F401\n",
    "print(\"Core imports OK (lean setup)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96272050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: Provider(kind='ollama', model='llama3.1:8b')\n"
     ]
    }
   ],
   "source": [
    "# 4) Configure Providers (Ollama first; optional OpenAI)\n",
    "import os, json, requests\n",
    "from dataclasses import dataclass\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "@dataclass\n",
    "class Provider:\n",
    "    kind: str  # 'ollama' or 'openai'\n",
    "    model: str\n",
    "\n",
    "def get_available_models():\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        if r.ok:\n",
    "            return {m.get('name') for m in r.json().get('models', []) if 'name' in m}\n",
    "    except Exception:\n",
    "        pass\n",
    "    return set()\n",
    "\n",
    "def pick_provider(prefer_ollama: bool=True) -> Provider:\n",
    "    if prefer_ollama:\n",
    "        installed = get_available_models()\n",
    "        # Default order for NEET evaluation: llama3.1:8b â†’ gemma2:9b â†’ mistral-nemo:12b\n",
    "        order = ['llama3.1:8b', 'gemma2:9b', 'mistral-nemo:12b']\n",
    "        for m in order:\n",
    "            if m and ((not installed) or (m in installed)):\n",
    "                return Provider('ollama', m)\n",
    "    if OPENAI_API_KEY:\n",
    "        return Provider('openai', 'gpt-4o-mini')\n",
    "    # Fallback\n",
    "    return Provider('ollama', 'llama3.1:8b')\n",
    "\n",
    "provider = pick_provider(True)\n",
    "print(\"Using provider:\", provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207e684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama reachable at http://localhost:11434\n",
      "Pull these tags for the evaluations (pick any/all):\n",
      " - ollama pull llama3.1:8b   # tools-capable local chat model (baseline)\n",
      " - ollama pull gemma2:9b     # higher quality candidate (more RAM)\n",
      " - ollama pull mistral-nemo:12b  # higher quality candidate (heavier)\n",
      " - ollama pull nomic-embed-text  # embeddings needed for vector DB\n",
      "MLflow not installed/available (optional): No module named 'mlflow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB client created via Settings. Persisting at: D:\\Git Projects\\SMU\\ANEETAA\\.chroma\n",
      "ChromaDB OK.\n"
     ]
    }
   ],
   "source": [
    "# 5) Start Services: Ollama, optional MLflow, ChromaDB\n",
    "import time, requests, os, subprocess, sys, pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Silence Chroma telemetry noise (set before importing chromadb)\n",
    "os.environ.setdefault(\"ANONYMIZED_TELEMETRY\", \"false\")\n",
    "os.environ.setdefault(\"CHROMA_TELEMETRY_ENABLED\", \"false\")\n",
    "\n",
    "\n",
    "def check_ollama(url=OLLAMA_URL):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        return r.status_code in (200, 404)  # 404 is OK for root\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "if not check_ollama():\n",
    "    print(\"Ollama not reachable at\", OLLAMA_URL)\n",
    "    print(\"- Install/start Ollama, then run: ollama serve\")\n",
    "else:\n",
    "    print(\"Ollama reachable at\", OLLAMA_URL)\n",
    "    print(\"Pull these tags for the evaluations (pick any/all):\")\n",
    "    print(\" - ollama pull llama3.1:8b   # tools-capable local chat model (baseline)\")\n",
    "    print(\" - ollama pull gemma2:9b     # higher quality candidate (more RAM)\")\n",
    "    print(\" - ollama pull mistral-nemo:12b  # higher quality candidate (heavier)\")\n",
    "    print(\" - ollama pull nomic-embed-text  # embeddings needed for vector DB\")\n",
    "\n",
    "# Optional MLflow setup (skip if not installed)\n",
    "try:\n",
    "    import mlflow  # noqa: F401\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://127.0.0.1:5000\"))\n",
    "    print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
    "    MLFLOW_OK = True\n",
    "except Exception as e:\n",
    "    print(\"MLflow not installed/available (optional):\", e)\n",
    "    MLFLOW_OK = False\n",
    "\n",
    "# Initialize a persistent Chroma client, reusing any existing kernel client to avoid settings conflicts\n",
    "import chromadb\n",
    "persist_dir = str((ROOT / \".chroma\").resolve())\n",
    "\n",
    "reuse = False\n",
    "if \"client\" in globals() and getattr(globals().get(\"client\"), \"get_or_create_collection\", None):\n",
    "    # Reuse existing client to avoid \"already exists with different settings\" errors\n",
    "    print(\"Reusing existing ChromaDB client. Persisting at:\", persist_dir)\n",
    "    reuse = True\n",
    "\n",
    "if not reuse:\n",
    "    try:\n",
    "        from chromadb.config import Settings\n",
    "        settings = Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "        client = chromadb.Client(settings)\n",
    "        print(\"ChromaDB client created via Settings. Persisting at:\", persist_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Settings-based client failed, falling back to PersistentClient:\", e)\n",
    "        client = chromadb.PersistentClient(path=persist_dir)\n",
    "        print(\"ChromaDB PersistentClient created. Persisting at:\", persist_dir)\n",
    "\n",
    "try:\n",
    "    _ = client.get_or_create_collection(\"smoke_test\")\n",
    "    print(\"ChromaDB OK.\")\n",
    "except ValueError as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(\"Chroma singleton already initialized with different settings; reusing existing client.\")\n",
    "    else:\n",
    "        print(\"ChromaDB init failed:\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7779bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonlines installed and imported\n"
     ]
    }
   ],
   "source": [
    "# HOTFIX: ensure jsonlines is available\n",
    "import sys, subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"jsonlines\"], check=True)\n",
    "import jsonlines; print(\"jsonlines installed and imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d9c3a",
   "metadata": {},
   "source": [
    "### Troubleshooting Ollama model pulls (Windows)\n",
    "If `ollama pull` fails with `pull model manifest: file does not exist`:\n",
    "\n",
    "1) Confirm the service is running\n",
    "- In a new PowerShell: `ollama serve` (if you see `bind: Only one usage...`, it means Ollama is already running; skip this.)\n",
    "- Check: `Invoke-WebRequest http://localhost:11434/ -UseBasicParsing`\n",
    "\n",
    "2) Use valid tags for your version\n",
    "- Try: `ollama pull llama3.1:8b`  \n",
    "- Or: `ollama pull gemma2:9b`\n",
    "- Or: `ollama pull mistral-nemo:12b`\n",
    "\n",
    "3) Update Ollama\n",
    "- Download the latest from https://ollama.com/download and reinstall.\n",
    "\n",
    "4) Clear local cache (optional)\n",
    "- Stop service; delete `%LOCALAPPDATA%\\Ollama\\models` cautiously; then `ollama serve` and pull again.\n",
    "\n",
    "Note: For ANEETA Doubt Solver, you DO need the embedding model `nomic-embed-text` to open the existing Chroma vector stores. Pull it with: `ollama pull nomic-embed-text`. Also prefer a tools-capable chat model like `llama3.1:8b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4439633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ANEETA repo processed data at: D:\\Git Projects\\SMU\\ANEETAA\\Processed Data\n",
      "Loaded rows: 8890 from: D:\\Git Projects\\SMU\\ANEETAA\\Processed Data\\solved_question_papers.json\n",
      "Loaded rows: 8890 from: D:\\Git Projects\\SMU\\ANEETAA\\Processed Data\\solved_question_papers.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neet:1:0</td>\n",
       "      <td>Two objects of mass 10 kg and 20 kg respective...</td>\n",
       "      <td></td>\n",
       "      <td>(3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neet:2:1</td>\n",
       "      <td>Match List-I with List-II \\n List-I \\n(Electro...</td>\n",
       "      <td></td>\n",
       "      <td>(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neet:3:2</td>\n",
       "      <td>The energy that will be ideally radiated by a ...</td>\n",
       "      <td></td>\n",
       "      <td>(2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           question context answer\n",
       "0  neet:1:0  Two objects of mass 10 kg and 20 kg respective...            (3)\n",
       "1  neet:2:1  Match List-I with List-II \\n List-I \\n(Electro...            (1)\n",
       "2  neet:3:2  The energy that will be ideally radiated by a ...            (2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Load ANEETA Dataset â†’ normalize to DataFrame (repo data only)\n",
    "import pandas as pd, json, os, re\n",
    "from pathlib import Path\n",
    "\n",
    "# Local convenience path (optional)\n",
    "local_processed = (ROOT / \"datasets\" / \"aneeta\" / \"processed\").resolve()\n",
    "# Authoritative repo path\n",
    "repo_processed = (aneeta_dir / \"Processed Data\").resolve()\n",
    "\n",
    "rows = []\n",
    "source_used = None\n",
    "\n",
    "# 1) Prefer local test.jsonl if present (explicit test set)\n",
    "test_path = local_processed / \"test.jsonl\"\n",
    "if test_path.exists():\n",
    "    import jsonlines\n",
    "    with jsonlines.open(test_path, 'r') as reader:\n",
    "        rows = list(reader)\n",
    "    source_used = str(test_path)\n",
    "\n",
    "# Helper: specialized parser for NEET solved papers (extract Q and numeric answer)\n",
    "def parse_solved_mcqs(json_path: Path):\n",
    "    try:\n",
    "        data = json.loads(json_path.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        return []\n",
    "    # Concatenate pages and search for patterns like: \"1. <question/body> ... Answer (3)\"\n",
    "    text = \"\\n\\n\".join([str(obj.get(\"page_content\", \"\")) for obj in data])\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    # Regex: capture question number, question block (non-greedy), and the numeric answer in parentheses\n",
    "    pat = re.compile(r\"(?:^|\\n)\\s*(\\d{1,3})\\.\\s*(.*?)(?:\\n\\s*Answer\\s*\\((\\d+)\\))\", re.S)\n",
    "    out = []\n",
    "    used_ids = set()\n",
    "    for i, m in enumerate(pat.finditer(text)):\n",
    "        qn = m.group(1) or str(i)\n",
    "        body = (m.group(2) or \"\").strip()\n",
    "        # Truncate at a solution marker if present to keep the stem/options only\n",
    "        body = body.split(\"\\nSol.\")[0].strip()\n",
    "        # Normalize whitespace\n",
    "        body = re.sub(r\"[ \\t]+\", \" \", body)\n",
    "        ans_num = m.group(3)\n",
    "        # Build a stable id even if question numbers repeat across sections\n",
    "        uid = f\"neet:{qn}:{i}\"\n",
    "        if uid in used_ids:\n",
    "            continue\n",
    "        used_ids.add(uid)\n",
    "        # Keep the whole body as the question (includes options like (1)...(4) when present)\n",
    "        out.append({\n",
    "            \"id\": uid,\n",
    "            \"question\": body,\n",
    "            \"context\": \"\",\n",
    "            \"answer\": f\"({ans_num})\"\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# 2) Otherwise, strictly load from ANEETA repo's Processed Data\n",
    "if not rows:\n",
    "    if not repo_processed.exists():\n",
    "        raise SystemExit(f\"ANEETA processed data folder not found at: {repo_processed}\")\n",
    "    print(\"Using ANEETA repo processed data at:\", repo_processed)\n",
    "\n",
    "    # First, try specialized parse from solved_question_papers.json for MCQ Q/A pairs\n",
    "    solved_path = repo_processed / \"solved_question_papers.json\"\n",
    "    if solved_path.exists():\n",
    "        parsed = parse_solved_mcqs(solved_path)\n",
    "        if parsed:\n",
    "            rows = parsed\n",
    "            source_used = str(solved_path)\n",
    "\n",
    "    # If still empty, fall back to generic extraction from other processed files (context only)\n",
    "    if not rows:\n",
    "        candidates = [\n",
    "            \"mentor_data.json\",\n",
    "            \"processed_biology_chunks.json\",\n",
    "            \"processed_chemistry_chunks.json\",\n",
    "            \"processed_physics_chunks.json\",\n",
    "        ]\n",
    "        def load_json(path: Path):\n",
    "            try:\n",
    "                data = json.loads(path.read_text(encoding='utf-8'))\n",
    "                if isinstance(data, dict):\n",
    "                    # flatten dict-of-lists if needed\n",
    "                    flat = []\n",
    "                    for v in data.values():\n",
    "                        if isinstance(v, list):\n",
    "                            flat.extend(v)\n",
    "                    return flat or [data]\n",
    "                return data if isinstance(data, list) else [data]\n",
    "            except Exception:\n",
    "                return []\n",
    "        def get_ci(d: dict, keys: list[str]):\n",
    "            # case-insensitive get\n",
    "            lk = {k.lower(): k for k in d.keys()}\n",
    "            for k in keys:\n",
    "                if k.lower() in lk:\n",
    "                    return d.get(lk[k.lower()])\n",
    "            return None\n",
    "        extracted = []\n",
    "        used_file = None\n",
    "        for name in candidates:\n",
    "            f = repo_processed / name\n",
    "            if not f.exists():\n",
    "                continue\n",
    "            data = load_json(f)\n",
    "            for i, rec in enumerate(data):\n",
    "                if not isinstance(rec, dict):\n",
    "                    continue\n",
    "                # These processed files generally have 'page_content' text chunks\n",
    "                ctx = get_ci(rec, [\"page_content\",\"content\",\"text\",\"chunk\"]) or \"\"\n",
    "                if isinstance(ctx, (dict, list)):\n",
    "                    ctx = json.dumps(ctx, ensure_ascii=False)\n",
    "                if isinstance(ctx, str) and len(ctx.strip()) >= 20:\n",
    "                    # Use the chunk as context and prompt the agent with a generic instruction\n",
    "                    q_text = ctx.strip().split(\"\\n\")[0]\n",
    "                    extracted.append({\n",
    "                        \"id\": f\"{name}:{i}\",\n",
    "                        \"question\": q_text[:500],\n",
    "                        \"context\": ctx[:2000],\n",
    "                        \"answer\": \"\"\n",
    "                    })\n",
    "            if extracted:\n",
    "                rows = extracted\n",
    "                used_file = f\n",
    "                break\n",
    "        source_used = str(used_file) if used_file else source_used\n",
    "\n",
    "# Build DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if df.empty:\n",
    "    raise SystemExit(\"Empty dataset after parsing ANEETA data.\")\n",
    "print(\"Loaded rows:\", len(df), \"from:\", source_used)\n",
    "\n",
    "# Normalize columns and order\n",
    "df = df.rename(columns={\"sample_id\":\"id\"})\n",
    "for col in [\"id\",\"question\",\"context\",\"answer\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "# Cap dataset size for quick iteration\n",
    "max_n = int(os.getenv(\"ANEETA_MAX_RECORDS\", \"250\"))\n",
    "df = df[[\"id\",\"question\",\"context\",\"answer\"]].head(max_n)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2e4fc",
   "metadata": {},
   "source": [
    "## Baseline ANEETA Doubt Solver (nonâ€‘DSPy)\n",
    "The following cells call ANEETA's mcq_question_solver_agent through the LangGraph workflow to provide a baseline Doubt Solver run without DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bfb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_MODEL: llama3.1:8b\n",
      "CREATIVE_LLM_MODEL: llama3.1:8b\n",
      "EMBEDDING_MODEL: nomic-embed-text\n",
      "ANEETA VECTORDB_BASE_PATH: src/aneeta/vectordb\n",
      "PYTHONPATH +: D:\\Git Projects\\SMU\\ANEETAA\\src\n",
      "Cleared streamlit cache_resource\n",
      "Cleared streamlit cache_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 00:25:21.785 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.117 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\benja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-31 00:25:22.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.117 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\benja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-31 00:25:22.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.619 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.621 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.621 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.619 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.621 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:22.621 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.914 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:46.914 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.238 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.238 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.238 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.238 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "D:\\Git Projects\\SMU\\ANEETAA\\src\\aneeta\\vectordb\\db_loader.py:29: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_stores[subject] = Chroma(persist_directory=persist_dir, embedding_function=_embeddings)\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "D:\\Git Projects\\SMU\\ANEETAA\\src\\aneeta\\vectordb\\db_loader.py:29: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_stores[subject] = Chroma(persist_directory=persist_dir, embedding_function=_embeddings)\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-10-31 00:25:47.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-10-31 00:25:47.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.534 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-31 00:25:47.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANEETA resources -> llm.model: llama3.1:8b\n",
      "Smoke test:\n",
      "Answer: (B) Mitochondria\n",
      "\n",
      "Explanation: The mitochondria are known as the \"powerhouse\" of the cell because they generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy. In addition to generating ATP, mitochondria are involved in a range of other processes, including signaling, cellular differentiation, cell death, as well as the control of the cell cycle and cell growth and division.\n",
      "\n",
      "Translation: à¤®à¤¾à¤‡à¤Ÿà¥‹à¤•à¥‰à¤¨à¥à¤¡à¥à¤°à¤¿à¤¯à¤¾ à¤•à¥‹à¤¶à¤¿à¤•à¤¾ à¤•à¤¾ à¤¶à¤•à¥à¤¤à¤¿ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¹à¥ˆà¥¤\n",
      "Answer: (B) Mitochondria\n",
      "\n",
      "Explanation: The mitochondria are known as the \"powerhouse\" of the cell because they generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy. In addition to generating ATP, mitochondria are involved in a range of other processes, including signaling, cellular differentiation, cell death, as well as the control of the cell cycle and cell growth and division.\n",
      "\n",
      "Translation: à¤®à¤¾à¤‡à¤Ÿà¥‹à¤•à¥‰à¤¨à¥à¤¡à¥à¤°à¤¿à¤¯à¤¾ à¤•à¥‹à¤¶à¤¿à¤•à¤¾ à¤•à¤¾ à¤¶à¤•à¥à¤¤à¤¿ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¹à¥ˆà¥¤\n"
     ]
    }
   ],
   "source": [
    "# Configure ANEETA environment (local-only)\n",
    "import os, sys, re\n",
    "\n",
    "# Force baseline model for NEET evaluation (override any environment defaults)\n",
    "os.environ[\"LLM_MODEL\"] = \"llama3.1:8b\"\n",
    "os.environ[\"CREATIVE_LLM_MODEL\"] = \"llama3.1:8b\"\n",
    "# Embeddings are required to open the persisted Chroma vector stores; use the model that matches the DBs\n",
    "os.environ.setdefault(\"EMBEDDING_MODEL\", \"nomic-embed-text\")\n",
    "# Use the ANEETA repo's persisted Chroma DBs\n",
    "os.environ.setdefault(\"VECTORDB_BASE_PATH\", str((aneeta_dir / \"src\" / \"aneeta\" / \"vectordb\").resolve()))\n",
    "\n",
    "print(\"LLM_MODEL:\", os.environ[\"LLM_MODEL\"]) \n",
    "print(\"CREATIVE_LLM_MODEL:\", os.environ[\"CREATIVE_LLM_MODEL\"]) \n",
    "print(\"EMBEDDING_MODEL:\", os.environ[\"EMBEDDING_MODEL\"])  # tip: if missing locally, run: ollama pull nomic-embed-text\n",
    "print(\"ANEETA VECTORDB_BASE_PATH:\", os.environ[\"VECTORDB_BASE_PATH\"])\n",
    "\n",
    "# Add ANEETA src to path for imports\n",
    "sys.path.insert(0, str((aneeta_dir / \"src\").resolve()))\n",
    "print(\"PYTHONPATH +:\", (aneeta_dir/\"src\").resolve())\n",
    "\n",
    "# Hard reset: clear Streamlit cache and fully purge ANEETA modules so model changes take effect\n",
    "import importlib\n",
    "try:\n",
    "    import streamlit as st\n",
    "    try:\n",
    "        st.cache_resource.clear()\n",
    "        print(\"Cleared streamlit cache_resource\")\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    st = None  # type: ignore\n",
    "\n",
    "# Purge ALL aneeta modules to avoid stale singletons (llm, resources, workflow, etc.)\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod == \"aneeta\" or mod.startswith(\"aneeta.\"):\n",
    "        sys.modules.pop(mod, None)\n",
    "\n",
    "# Import in a clean order: LLM -> resources -> agents/router -> workflow\n",
    "import aneeta.llm.llm as llm_mod\n",
    "importlib.reload(llm_mod)\n",
    "\n",
    "import aneeta.core.resources as resources_mod\n",
    "importlib.reload(resources_mod)\n",
    "\n",
    "import aneeta.nodes.agents as agents_mod\n",
    "import aneeta.nodes.router as router_mod\n",
    "importlib.reload(agents_mod)\n",
    "importlib.reload(router_mod)\n",
    "\n",
    "import aneeta.graph.workflow as workflow_mod\n",
    "importlib.reload(workflow_mod)\n",
    "\n",
    "# Print resolved model from ANEETA resources for verification\n",
    "try:\n",
    "    from aneeta.core.resources import llm as current_llm\n",
    "    resolved_model = getattr(current_llm, \"model\", None) or getattr(current_llm, \"model_name\", None) or getattr(current_llm, \"model_id\", None)\n",
    "    print(\"ANEETA resources -> llm.model:\", resolved_model)\n",
    "except Exception as e:\n",
    "    print(\"Could not read ANEETA resources llm model:\", e)\n",
    "\n",
    "# Define a simple whitespace normalizer to clean streamed outputs\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    t = text.replace(\"\\r\", \"\")\n",
    "    # Collapse 3+ newlines to 2\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    # Collapse runs of spaces\n",
    "    t = re.sub(r\" {2,}\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "# Define baseline solve_mcq using ANEETA LangGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def solve_mcq(question: str, timeout: int = 120, language: str | None = None):\n",
    "    try:\n",
    "        graph = workflow_mod.get_graph()\n",
    "        init_state = {\n",
    "            \"messages\": [HumanMessage(content=question)],\n",
    "            \"user_explanation_language\": language or os.getenv(\"ANEETA_EXPLANATION_LANG\", \"English\"),\n",
    "        }\n",
    "        # Stream and collect final text (supports either messages or response_stream)\n",
    "        final_text = []\n",
    "        for update in graph.stream(init_state):\n",
    "            for _, v in update.items():\n",
    "                if isinstance(v, dict):\n",
    "                    # If agent provided a streaming generator, consume it\n",
    "                    rs = v.get(\"response_stream\")\n",
    "                    if rs is not None:\n",
    "                        try:\n",
    "                            for chunk in rs:\n",
    "                                # Don't strip() here to preserve word boundaries\n",
    "                                t = str(getattr(chunk, \"content\", chunk))\n",
    "                                if t:\n",
    "                                    final_text.append(t)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    # Also collect from messages if present\n",
    "                    if \"messages\" in v:\n",
    "                        msgs = v.get(\"messages\") or []\n",
    "                        if msgs and hasattr(msgs[-1], \"content\"):\n",
    "                            final_text.append(str(msgs[-1].content))\n",
    "        # Join with empty string to avoid breaking words\n",
    "        raw_text = \"\".join(final_text).strip()\n",
    "        return {\"text\": normalize_whitespace(raw_text)}\n",
    "    except Exception as e:\n",
    "        return {\"text\": f\"[baseline error: {e}]\"}\n",
    "\n",
    "# Smoke test (removed [:300] truncation to show full output)\n",
    "test_q = \"Which organelle is the powerhouse of the cell? Options: (A) Ribosome (B) Mitochondria (C) Golgi (D) Lysosome\"\n",
    "print(\"Smoke test:\")\n",
    "print(solve_mcq(test_q).get(\"text\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da468ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>latency_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>2855.539799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>3219.932795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>3339.609146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model   latency_ms\n",
       "0       llama3.1:8b  2855.539799\n",
       "1  mistral-nemo:12b  3219.932795\n",
       "2         gemma2:9b  3339.609146"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick model switch micro-benchmark (local)\n",
    "# This is optional; uses installed models only\n",
    "try:\n",
    "    r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
    "    names = [m.get('name') for m in r.json().get('models', [])]\n",
    "except Exception:\n",
    "    names = []\n",
    "\n",
    "# Shortlist: baseline + two higher-quality candidates\n",
    "model_shortlist = [\"llama3.1:8b\", \"gemma2:9b\", \"mistral-nemo:12b\"]\n",
    "preferred = [m for m in model_shortlist if (not names) or (m in names)]\n",
    "if not preferred:\n",
    "    preferred = [\"llama3.1:8b\"]  # safe default\n",
    "\n",
    "benchmarks = []\n",
    "for name in preferred:\n",
    "    os.environ[\"LLM_MODEL\"] = name\n",
    "    s = time.time(); _ = solve_mcq(\"During DNA replication, the enzyme responsible for joining Okazaki fragments is: Options: (A) DNA polymerase (B) DNA ligase (C) Helicase (D) Primase\"); dt = (time.time()-s)*1000\n",
    "    benchmarks.append({\"model\": name, \"latency_ms\": dt})\n",
    "\n",
    "pd.DataFrame(benchmarks).sort_values(\"latency_ms\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a345fb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth answers from ANEETA repository...\n",
      "âœ“ Loaded 1938 ground truth Q&A pairs\n",
      "\n",
      "============================================================\n",
      "Evaluating models on 3 NEET MCQs with comprehensive metrics\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Testing llama3.1:8b:\n",
      "  - Physics... âœ“ (fact:10/10, quality:9/10, latency:4849ms)\n",
      "  - Chemistry... âœ“ (fact:10/10, quality:9/10, latency:4849ms)\n",
      "  - Chemistry... âœ“ (fact:2/10, quality:9/10, latency:4987ms)\n",
      "  - Biology... âœ“ (fact:2/10, quality:9/10, latency:4987ms)\n",
      "  - Biology... âœ“ (fact:10/10, quality:9/10, latency:3133ms)\n",
      "\n",
      "ðŸ“Š Testing gemma2:9b:\n",
      "  - Physics... âœ“ (fact:10/10, quality:9/10, latency:3133ms)\n",
      "\n",
      "ðŸ“Š Testing gemma2:9b:\n",
      "  - Physics... âœ“ (fact:10/10, quality:9/10, latency:4262ms)\n",
      "  - Chemistry... âœ“ (fact:10/10, quality:9/10, latency:4262ms)\n",
      "  - Chemistry... âœ“ (fact:2/10, quality:9/10, latency:4587ms)\n",
      "  - Biology... âœ“ (fact:2/10, quality:9/10, latency:4587ms)\n",
      "  - Biology... âœ“ (fact:10/10, quality:9/10, latency:2837ms)\n",
      "\n",
      "ðŸ“Š Testing mistral-nemo:12b:\n",
      "  - Physics... âœ“ (fact:10/10, quality:9/10, latency:2837ms)\n",
      "\n",
      "ðŸ“Š Testing mistral-nemo:12b:\n",
      "  - Physics... âœ“ (fact:10/10, quality:8/10, latency:4698ms)\n",
      "  - Chemistry... âœ“ (fact:10/10, quality:8/10, latency:4698ms)\n",
      "  - Chemistry... âœ“ (fact:2/10, quality:9/10, latency:5285ms)\n",
      "  - Biology... âœ“ (fact:2/10, quality:9/10, latency:5285ms)\n",
      "  - Biology... âœ“ (fact:10/10, quality:9/10, latency:3247ms)\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY: Model Performance Comparison\n",
      "============================================================\n",
      "âœ“ (fact:10/10, quality:9/10, latency:3247ms)\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY: Model Performance Comparison\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg_Latency_ms</th>\n",
       "      <th>Avg_Fact_Score</th>\n",
       "      <th>Avg_Quality_Score</th>\n",
       "      <th>Overall_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>4323.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>3895.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>4409.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>8.7</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Avg_Latency_ms  Avg_Fact_Score  Avg_Quality_Score  \\\n",
       "0       llama3.1:8b          4323.0             7.3                9.0   \n",
       "1         gemma2:9b          3895.3             7.3                9.0   \n",
       "2  mistral-nemo:12b          4409.8             7.3                8.7   \n",
       "\n",
       "   Overall_Score  \n",
       "0            8.2  \n",
       "1            8.2  \n",
       "2            8.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED: Per-Question Results\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Question</th>\n",
       "      <th>Latency_ms</th>\n",
       "      <th>Fact_Score</th>\n",
       "      <th>Quality_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>Q1 (Physics)</td>\n",
       "      <td>4849.1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>Q2 (Chemistry)</td>\n",
       "      <td>4986.8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>Q3 (Biology)</td>\n",
       "      <td>3133.2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>Q1 (Physics)</td>\n",
       "      <td>4261.9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>Q2 (Chemistry)</td>\n",
       "      <td>4587.3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>Q3 (Biology)</td>\n",
       "      <td>2836.6</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>Q1 (Physics)</td>\n",
       "      <td>4698.2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>Q2 (Chemistry)</td>\n",
       "      <td>5284.7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>Q3 (Biology)</td>\n",
       "      <td>3246.6</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model        Question  Latency_ms  Fact_Score  Quality_Score\n",
       "0       llama3.1:8b    Q1 (Physics)      4849.1          10              9\n",
       "1       llama3.1:8b  Q2 (Chemistry)      4986.8           2              9\n",
       "2       llama3.1:8b    Q3 (Biology)      3133.2          10              9\n",
       "3         gemma2:9b    Q1 (Physics)      4261.9          10              9\n",
       "4         gemma2:9b  Q2 (Chemistry)      4587.3           2              9\n",
       "5         gemma2:9b    Q3 (Biology)      2836.6          10              9\n",
       "6  mistral-nemo:12b    Q1 (Physics)      4698.2          10              8\n",
       "7  mistral-nemo:12b  Q2 (Chemistry)      5284.7           2              9\n",
       "8  mistral-nemo:12b    Q3 (Biology)      3246.6          10              9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate 3 MCQs across different local models with comprehensive metrics\n",
    "import os, numpy as np, pandas as pd, time, requests, json, re\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = lambda x: print(x)  # fallback\n",
    "\n",
    "# Quick reachability check to avoid confusing errors\n",
    "def _ollama_ok(url: str) -> bool:\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        return r.status_code in (200, 404)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if not _ollama_ok(OLLAMA_URL):\n",
    "    print(\"Ollama not reachable at\", OLLAMA_URL)\n",
    "    print(\"Start the service in a PowerShell:\")\n",
    "    print(\"ollama serve    # then rerun this cell\")\n",
    "else:\n",
    "    # Helper: get available Ollama tags to filter the test set\n",
    "    available = set()\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
    "        if r.ok:\n",
    "            for m in r.json().get('models', []):\n",
    "                if 'name' in m:\n",
    "                    available.add(m['name'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Shortlist for one-click comparisons\n",
    "    model_shortlist = [\"llama3.1:8b\", \"gemma2:9b\", \"mistral-nemo:12b\"]\n",
    "    models_to_test = [m for m in model_shortlist if (not available) or (m in available)]\n",
    "    missing = [m for m in model_shortlist if m not in models_to_test]\n",
    "    if missing:\n",
    "        print(\"Missing models not found in Ollama; pull as needed:\")\n",
    "        for m in missing:\n",
    "            print(f\" - ollama pull {m}\")\n",
    "    if not models_to_test:\n",
    "        models_to_test = [\"llama3.1:8b\"]  # safe default\n",
    "\n",
    "    # Load ground truth from ANEETA processed data\n",
    "    print(\"Loading ground truth answers from ANEETA repository...\")\n",
    "    solved_papers_path = aneeta_dir / \"Processed Data\" / \"solved_question_papers.json\"\n",
    "    ground_truth_map = {}\n",
    "    \n",
    "    if solved_papers_path.exists():\n",
    "        try:\n",
    "            data = json.loads(solved_papers_path.read_text(encoding='utf-8'))\n",
    "            # Build a simple question -> answer mapping\n",
    "            text = \"\\n\\n\".join([str(obj.get(\"page_content\", \"\")) for obj in data])\n",
    "            # Extract questions and their answers\n",
    "            pattern = re.compile(r\"(\\d{1,3})\\.\\s*(.*?)(?:\\n\\s*Answer\\s*\\((\\d+)\\))\", re.S)\n",
    "            for match in pattern.finditer(text):\n",
    "                q_num = match.group(1)\n",
    "                q_body = match.group(2).strip().split(\"\\nSol.\")[0].strip()\n",
    "                answer_num = match.group(3)\n",
    "                # Store first 100 chars of question as key\n",
    "                key = q_body[:100].lower()\n",
    "                ground_truth_map[key] = f\"({answer_num})\"\n",
    "            print(f\"âœ“ Loaded {len(ground_truth_map)} ground truth Q&A pairs\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Could not load ground truth: {e}\")\n",
    "    else:\n",
    "        print(f\"âš  Ground truth file not found at {solved_papers_path}\")\n",
    "\n",
    "    # NEET-aligned MCQ examples with ground truth answers\n",
    "    mcq_examples = [\n",
    "        {\n",
    "            \"subject\": \"Physics\",\n",
    "            \"question\": \"A body moving in a circle of radius r with speed v has centripetal acceleration? Options: (A) v^2/r (B) r/v^2 (C) v/r^2 (D) r^2/v\",\n",
    "            \"correct_answer\": \"(A)\"  # v^2/r is correct\n",
    "        },\n",
    "        {\n",
    "            \"subject\": \"Chemistry\", \n",
    "            \"question\": \"If 2 mol of an ideal gas at constant temperature compress to half the volume, what happens to pressure? Options: (A) doubles (B) halves (C) same (D) zero\",\n",
    "            \"correct_answer\": \"(A)\"  # Boyle's law: P1V1 = P2V2, so P doubles\n",
    "        },\n",
    "        {\n",
    "            \"subject\": \"Biology\",\n",
    "            \"question\": \"During DNA replication, the enzyme responsible for joining Okazaki fragments is: Options: (A) DNA polymerase (B) DNA ligase (C) Helicase (D) Primase\",\n",
    "            \"correct_answer\": \"(B)\"  # DNA ligase joins Okazaki fragments\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # LLM Judge function (uses llama3.1:8b as judge)\n",
    "    def judge_answer_quality(question: str, answer: str, judge_model: str = \"llama3.1:8b\") -> dict:\n",
    "        \"\"\"\n",
    "        Uses an LLM to judge answer quality on clarity, reasoning steps, and explanation quality.\n",
    "        Returns score 1-10 and reasoning.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from langchain_ollama import ChatOllama\n",
    "            judge_llm = ChatOllama(model=judge_model, base_url=OLLAMA_URL)\n",
    "            \n",
    "            judge_prompt = f\"\"\"You are evaluating a student's answer to a NEET exam question. Rate the answer quality on a scale of 1-10 based on:\n",
    "- Clarity of explanation (is it easy to understand?)\n",
    "- Logical reasoning steps (does it show clear step-by-step thinking?)\n",
    "- Correctness of approach (even if final answer is wrong, is the reasoning sound?)\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Student's Answer:\n",
    "{answer}\n",
    "\n",
    "Provide your evaluation in this exact format:\n",
    "Score: [number 1-10]\n",
    "Reasoning: [brief explanation of your score]\n",
    "\n",
    "Your evaluation:\"\"\"\n",
    "            \n",
    "            response = judge_llm.invoke(judge_prompt)\n",
    "            response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            # Extract score\n",
    "            score_match = re.search(r\"Score:\\s*(\\d+)\", response_text)\n",
    "            score = int(score_match.group(1)) if score_match else 5\n",
    "            score = max(1, min(10, score))  # Clamp to 1-10\n",
    "            \n",
    "            # Extract reasoning\n",
    "            reasoning_match = re.search(r\"Reasoning:\\s*(.+)\", response_text, re.S)\n",
    "            reasoning = reasoning_match.group(1).strip()[:200] if reasoning_match else \"No reasoning provided\"\n",
    "            \n",
    "            return {\"score\": score, \"reasoning\": reasoning}\n",
    "        except Exception as e:\n",
    "            return {\"score\": 5, \"reasoning\": f\"Judge error: {str(e)[:100]}\"}\n",
    "\n",
    "    # Fact-check function\n",
    "    def fact_check_answer(model_answer: str, correct_answer: str) -> int:\n",
    "        \"\"\"\n",
    "        Compares model answer against ground truth.\n",
    "        Returns score 0-10 based on answer correctness.\n",
    "        \"\"\"\n",
    "        # Extract the option letter from model answer (look for (A), (B), (C), (D), etc.)\n",
    "        model_option = re.search(r\"\\(([A-D1-4])\\)\", model_answer)\n",
    "        model_option = model_option.group(1) if model_option else None\n",
    "        \n",
    "        correct_option = re.search(r\"\\(([A-D1-4])\\)\", correct_answer)\n",
    "        correct_option = correct_option.group(1) if correct_option else None\n",
    "        \n",
    "        if model_option and correct_option:\n",
    "            return 10 if model_option == correct_option else 0\n",
    "        else:\n",
    "            # Fuzzy match: check if correct answer appears in response\n",
    "            return 7 if correct_answer.lower() in model_answer.lower() else 2\n",
    "\n",
    "    def eval_model_comprehensive(model_name: str):\n",
    "        os.environ[\"LLM_MODEL\"] = model_name\n",
    "        results = []\n",
    "        latencies = []\n",
    "        \n",
    "        for mcq in mcq_examples:\n",
    "            print(f\"  - {mcq['subject']}...\", end=\" \", flush=True)\n",
    "            t0 = time.time()\n",
    "            out = solve_mcq(mcq[\"question\"])\n",
    "            dt = (time.time()-t0)*1000\n",
    "            latencies.append(dt)\n",
    "            \n",
    "            answer_text = out.get(\"text\",\"\")\n",
    "            answer_summary = answer_text.split(\"\\n\")[0][:200] if answer_text else \"[no response]\"\n",
    "            \n",
    "            # Fact-check score\n",
    "            fact_score = fact_check_answer(answer_text, mcq[\"correct_answer\"])\n",
    "            \n",
    "            # Quality score (only judge if model produced an answer)\n",
    "            if answer_text and len(answer_text) > 10:\n",
    "                quality_result = judge_answer_quality(mcq[\"question\"], answer_text)\n",
    "                quality_score = quality_result[\"score\"]\n",
    "                quality_reasoning = quality_result[\"reasoning\"]\n",
    "            else:\n",
    "                quality_score = 1\n",
    "                quality_reasoning = \"No meaningful answer provided\"\n",
    "            \n",
    "            print(f\"âœ“ (fact:{fact_score}/10, quality:{quality_score}/10, latency:{dt:.0f}ms)\")\n",
    "            \n",
    "            results.append({\n",
    "                \"subject\": mcq[\"subject\"],\n",
    "                \"answer\": answer_summary,\n",
    "                \"latency_ms\": round(dt, 1),\n",
    "                \"fact_check_score\": fact_score,\n",
    "                \"quality_score\": quality_score,\n",
    "                \"quality_reasoning\": quality_reasoning\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"avg_latency_ms\": round(np.mean(latencies), 1),\n",
    "            \"avg_fact_score\": round(np.mean([r[\"fact_check_score\"] for r in results]), 1),\n",
    "            \"avg_quality_score\": round(np.mean([r[\"quality_score\"] for r in results]), 1),\n",
    "            \"results\": results\n",
    "        }\n",
    "\n",
    "    # Evaluate all models\n",
    "    all_results = []\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating models on 3 NEET MCQs with comprehensive metrics\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for m in models_to_test:\n",
    "        print(f\"ðŸ“Š Testing {m}:\")\n",
    "        eval_result = eval_model_comprehensive(m)\n",
    "        all_results.append(eval_result)\n",
    "        print()\n",
    "    \n",
    "    # Build detailed DataFrame\n",
    "    rows = []\n",
    "    for result in all_results:\n",
    "        for i, mcq_result in enumerate(result[\"results\"], 1):\n",
    "            rows.append({\n",
    "                \"Model\": result[\"model\"],\n",
    "                \"Question\": f\"Q{i} ({mcq_result['subject']})\",\n",
    "                \"Answer\": mcq_result[\"answer\"],\n",
    "                \"Latency_ms\": mcq_result[\"latency_ms\"],\n",
    "                \"Fact_Score\": mcq_result[\"fact_check_score\"],\n",
    "                \"Quality_Score\": mcq_result[\"quality_score\"],\n",
    "                \"Quality_Reasoning\": mcq_result[\"quality_reasoning\"]\n",
    "            })\n",
    "    \n",
    "    df_detailed = pd.DataFrame(rows)\n",
    "    \n",
    "    # Summary DataFrame\n",
    "    summary_rows = []\n",
    "    for result in all_results:\n",
    "        summary_rows.append({\n",
    "            \"Model\": result[\"model\"],\n",
    "            \"Avg_Latency_ms\": result[\"avg_latency_ms\"],\n",
    "            \"Avg_Fact_Score\": result[\"avg_fact_score\"],\n",
    "            \"Avg_Quality_Score\": result[\"avg_quality_score\"],\n",
    "            \"Overall_Score\": round((result[\"avg_fact_score\"] + result[\"avg_quality_score\"]) / 2, 1)\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_rows).sort_values(\"Overall_Score\", ascending=False)\n",
    "    \n",
    "    # Optional MLflow logging\n",
    "    try:\n",
    "        import mlflow\n",
    "        mlflow.set_experiment(\"aneeta-baseline-doubtsolver-comprehensive\")\n",
    "        for result in all_results:\n",
    "            with mlflow.start_run(run_name=f\"baseline_{result['model']}\"):\n",
    "                mlflow.log_params({\"model\": result[\"model\"], \"agent\": \"mcq_question_solver\"})\n",
    "                mlflow.log_metrics({\n",
    "                    \"avg_latency_ms\": result[\"avg_latency_ms\"],\n",
    "                    \"avg_fact_score\": result[\"avg_fact_score\"],\n",
    "                    \"avg_quality_score\": result[\"avg_quality_score\"]\n",
    "                })\n",
    "        print(\"âœ“ Logged to MLflow\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY: Model Performance Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    display(df_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED: Per-Question Results\")\n",
    "    print(\"=\"*60)\n",
    "    display(df_detailed[[\"Model\", \"Question\", \"Latency_ms\", \"Fact_Score\", \"Quality_Score\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
