{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa602d14-6372-4e0a-92d8-6f7db048c3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DSPy Quickstart\n",
    "\n",
    "[DSPy](https://dspy-docs.vercel.app/) simplifies building language model (LM) pipelines by replacing manual prompt engineering with structured \"text transformation graphs.\" These graphs use flexible, learning modules that automate and optimize LM tasks like reasoning, retrieval, and answering complex questions. \n",
    "\n",
    "## How does it work?\n",
    "At a high level, DSPy optimizes prompts, selects the best language model, and can even fine-tune the model using training data.\n",
    "\n",
    "The process follows these three steps, common to most DSPy [optimizers](https://dspy.ai/learn/optimization/optimizers/):\n",
    "\n",
    "1. **Candidate Generation**: DSPy finds all `Predict` modules in the program and generates variations of instructions and demonstrations (e.g., examples for prompts). This step creates a set of possible candidates for the next stage.\n",
    "2. **Parameter Optimization**: DSPy then uses methods like random search, TPE, or Optuna to select the best candidate. Fine-tuning models can also be done at this stage.\n",
    "\n",
    "## This Demo\n",
    "Below we create a simple program that demonstrates the power of DSPy. We will build a text classifier leveraging OpenAI. By the end of this tutorial, we will...\n",
    "\n",
    "1. Define a [dspy.Signature](https://dspy.ai/learn/programming/signatures/) and [dspy.Module](https://dspy.ai/learn/programming/modules/) to perform text classification.\n",
    "2. Leverage [dspy.SIMBA](https://dspy.ai/api/optimizers/SIMBA/) to compile our module so it's better at classifying our text.\n",
    "3. Analyze internal steps with MLflow Tracing.\n",
    "3. Log the compiled model with MLflow.\n",
    "4. Load the logged model and perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58918566-6e93-4a2e-9a34-b0f56378885a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\benja\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Collecting openai\n",
      "  Downloading openai-2.6.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: dspy>=3.0.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: mlflow>=3.4.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\benja\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\benja\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\benja\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\benja\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\benja\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from openai) (1.10.24)\n",
      "Requirement already satisfied: backoff>=2.2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (1.5.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (2025.10.23)\n",
      "Requirement already satisfied: orjson>=3.9.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (3.11.3)\n",
      "Requirement already satisfied: optuna>=3.4.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (4.5.0)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (0.1.6)\n",
      "Requirement already satisfied: litellm>=1.64.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (1.78.7)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (0.52.3)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (8.5.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (3.1.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from dspy>=3.0.3) (14.1.0)\n",
      "Requirement already satisfied: gepa==0.0.7 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from gepa[dspy]==0.0.7->dspy>=3.0.3) (0.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.41.4-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: mlflow-skinny==3.5.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (3.5.1)\n",
      "Requirement already satisfied: mlflow-tracing==3.5.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (3.5.1)\n",
      "Requirement already satisfied: Flask-CORS<7 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (5.0.0)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (3.1.0)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (1.15.1)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (46.0.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (3.4.3)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (3.7.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (1.3.0)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (1.10.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (2.0.39)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow>=3.4.0) (3.0.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (8.1.8)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.70.0)\n",
      "Requirement already satisfied: fastapi<1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.118.2)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (3.1.45)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (1.37.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (5.29.3)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (1.1.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.5.3)\n",
      "Requirement already satisfied: uvicorn<1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.37.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\benja\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow>=3.4.0) (1.3.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\benja\\anaconda3\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from cryptography<47,>=43.0.0->mlflow>=3.4.0) (2.0.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (2.38.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow>=3.4.0) (305.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow>=3.4.0) (1.26.16)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.48.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=3.4.0) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=3.4.0) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=3.4.0) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=3.4.0) (1.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.5.1->mlflow>=3.4.0) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.5.1->mlflow>=3.4.0) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (4.9)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=3.4.0) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=3.4.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=3.4.0) (2.8.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=3.4.0) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=3.4.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=3.4.0) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=3.4.0) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=3.4.0) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=3.4.0) (3.0.9)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.58b0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=3.4.0) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow>=3.4.0) (0.4.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow>=3.4.0) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=3.4.0) (2.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\benja\\anaconda3\\lib\\site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=3.4.0) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->Flask<4->mlflow>=3.4.0) (2.1.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from litellm>=1.64.0->dspy>=3.0.3) (0.14.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from litellm>=1.64.0->dspy>=3.0.3) (4.25.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from litellm>=1.64.0->dspy>=3.0.3) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\benja\\anaconda3\\lib\\site-packages (from litellm>=1.64.0->dspy>=3.0.3) (0.22.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.3) (0.28.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\benja\\anaconda3\\lib\\site-packages (from optuna>=3.4.0->dspy>=3.0.3) (6.10.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from rich>=13.7.1->dspy>=3.0.3) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from rich>=13.7.1->dspy>=3.0.3) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\benja\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.3) (0.1.0)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading openai-2.6.1-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.0 MB/s  0:00:00\n",
      "Using cached pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 28.4 MB/s  0:00:00\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, pyarrow, dill, typing-inspection, pydantic-core, multiprocess, pydantic, openai, datasets\n",
      "\n",
      "  Attempting uninstall: typing-extensions\n",
      "\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "  Attempting uninstall: pyarrow\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "   ---------------------------------------- 0/9 [typing-extensions]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "  Attempting uninstall: dill\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "    Found existing installation: dill 0.3.6\n",
      "   ---- ----------------------------------- 1/9 [pyarrow]\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "    Uninstalling dill-0.3.6:\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "  Attempting uninstall: typing-inspection\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "    Found existing installation: typing-inspection 0.4.1\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "    Uninstalling typing-inspection-0.4.1:\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "      Successfully uninstalled typing-inspection-0.4.1\n",
      "   -------- ------------------------------- 2/9 [dill]\n",
      "   ------------- -------------------------- 3/9 [typing-inspection]\n",
      "  Attempting uninstall: pydantic-core\n",
      "   ------------- -------------------------- 3/9 [typing-inspection]\n",
      "    Found existing installation: pydantic_core 2.33.2\n",
      "   ------------- -------------------------- 3/9 [typing-inspection]\n",
      "    Uninstalling pydantic_core-2.33.2:\n",
      "   ------------- -------------------------- 3/9 [typing-inspection]\n",
      "      Successfully uninstalled pydantic_core-2.33.2\n",
      "   ------------- -------------------------- 3/9 [typing-inspection]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ----------------- ---------------------- 4/9 [pydantic-core]\n",
      "   ---------------------- ----------------- 5/9 [multiprocess]\n",
      "   ---------------------- ----------------- 5/9 [multiprocess]\n",
      "  Attempting uninstall: pydantic\n",
      "   ---------------------- ----------------- 5/9 [multiprocess]\n",
      "    Found existing installation: pydantic 1.10.24\n",
      "   ---------------------- ----------------- 5/9 [multiprocess]\n",
      "    Uninstalling pydantic-1.10.24:\n",
      "   ---------------------- ----------------- 5/9 [multiprocess]\n",
      "      Successfully uninstalled pydantic-1.10.24\n",
      "   ---------------------- ----------------- 5/9 [multiprocess]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "  Attempting uninstall: openai\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "    Found existing installation: openai 2.2.0\n",
      "   -------------------------- ------------- 6/9 [pydantic]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "    Uninstalling openai-2.2.0:\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "      Successfully uninstalled openai-2.2.0\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ------------------------------- -------- 7/9 [openai]\n",
      "   ----------------------------------- ---- 8/9 [datasets]\n",
      "   ----------------------------------- ---- 8/9 [datasets]\n",
      "   ----------------------------------- ---- 8/9 [datasets]\n",
      "   ----------------------------------- ---- 8/9 [datasets]\n",
      "   ----------------------------------- ---- 8/9 [datasets]\n",
      "   ---------------------------------------- 9/9 [datasets]\n",
      "\n",
      "Successfully installed datasets-4.3.0 dill-0.4.0 multiprocess-0.70.16 openai-2.6.1 pyarrow-21.0.0 pydantic-2.12.3 pydantic-core-2.41.4 typing-extensions-4.15.0 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\benja\\anaconda3\\Lib\\site-packages\\~ydantic_core'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\benja\\anaconda3\\Lib\\site-packages\\~ydantic'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.11.0 which is incompatible.\n",
      "langchain-openai 0.3.35 requires langchain-core<1.0.0,>=0.3.78, but you have langchain-core 0.2.43 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets openai \"dspy>=3.0.3\" \"mlflow>=3.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd9e460-4c9c-4508-8801-6f29fcf2c8d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27ec364-1195-447d-a9d5-f38defa6652e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set Up LLM\n",
    "\n",
    "After installing the relevant dependencies, let's set up access to an OpenAI LLM. Here, will leverage OpenAI's `gpt-4o-mini` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API Key to the environment variable. You can also pass the token to dspy.LM()\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3232fb03-f4be-490f-9179-0f2b71129196",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Define your model. We will use OpenAI for simplicity\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Note that an OPENAI_API_KEY environment must be present. You can also pass the token to dspy.LM()\n",
    "lm = dspy.LM(\n",
    "    model=f\"openai/{model_name}\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.1,\n",
    ")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MLflow Experiment\n",
    "\n",
    "Create a new MLflow Experiment to track your DSPy models, metrics, parameters, and traces in one place. Although there is already a \"default\" experiment created in your workspace, it is highly recommended to create one for different tasks to organize experiment artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"DSPy Quickstart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn on Auto Tracing with MLflow\n",
    "\n",
    "[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) is a powerful observability tool for monitoring and debugging what happens inside your DSPy modules, helping you identify potential bottlenecks or issues quickly. To enable DSPy tracing, you just need to call `mlflow.dspy.autolog` and that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.dspy.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fde40f-650a-4090-9791-120dd954cd36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set Up Data\n",
    "\n",
    "Next, we will download the [Reuters 21578](https://huggingface.co/datasets/yangwang825/reuters-21578) dataset from Huggingface. We also write a utility to ensure that our train/test split has the same labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd53a405-1685-4e2f-86c5-8f3f570828c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 8\n",
      "Train labels: {'interest', 'earn', 'money-fx', 'trade', 'ship', 'grain', 'acq', 'crude'}\n",
      "Example({'text': 'bankamerica bacp raises prime rate to pct bankamerica corp following moves by other major banks said it has raised its prime rate to pct from pct effective today reuter', 'label': 'interest'}) (input_keys={'text'})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dspy.datasets.dataset import Dataset\n",
    "\n",
    "\n",
    "def read_data_and_subset_to_categories() -> tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read the reuters-21578 dataset. Docs can be found in the url below:\n",
    "    https://huggingface.co/datasets/yangwang825/reuters-21578\n",
    "    \"\"\"\n",
    "\n",
    "    # Read train/test split\n",
    "    dataset = load_dataset(\"yangwang825/reuters-21578\")\n",
    "    train = pd.DataFrame(dataset[\"train\"])\n",
    "    test = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    # Clean the labels\n",
    "    label_map = {\n",
    "        0: \"acq\",\n",
    "        1: \"crude\",\n",
    "        2: \"earn\",\n",
    "        3: \"grain\",\n",
    "        4: \"interest\",\n",
    "        5: \"money-fx\",\n",
    "        6: \"ship\",\n",
    "        7: \"trade\",\n",
    "    }\n",
    "\n",
    "    train[\"label\"] = train[\"label\"].map(label_map)\n",
    "    test[\"label\"] = test[\"label\"].map(label_map)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, n_train_per_label: int = 20, n_test_per_label: int = 10, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_train_per_label = n_train_per_label\n",
    "        self.n_test_per_label = n_test_per_label\n",
    "\n",
    "        self._create_train_test_split_and_ensure_labels()\n",
    "\n",
    "    def _create_train_test_split_and_ensure_labels(self) -> None:\n",
    "        \"\"\"Perform a train/test split that ensure labels in `dev` are also in `train`.\"\"\"\n",
    "        # Read the data\n",
    "        train_df, test_df = read_data_and_subset_to_categories()\n",
    "\n",
    "        # Sample for each label\n",
    "        train_samples_df = pd.concat(\n",
    "            [group.sample(n=self.n_train_per_label) for _, group in train_df.groupby(\"label\")]\n",
    "        )\n",
    "        test_samples_df = pd.concat(\n",
    "            [group.sample(n=self.n_test_per_label) for _, group in test_df.groupby(\"label\")]\n",
    "        )\n",
    "\n",
    "        # Set DSPy class variables\n",
    "        self._train = train_samples_df.to_dict(orient=\"records\")\n",
    "        self._dev = test_samples_df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# Limit to a small dataset to showcase the value of bootstrapping\n",
    "dataset = CSVDataset(n_train_per_label=3, n_test_per_label=1)\n",
    "\n",
    "# Create train and test sets containing DSPy\n",
    "# Note that we must specify the expected input value name\n",
    "train_dataset = [example.with_inputs(\"text\") for example in dataset.train]\n",
    "test_dataset = [example.with_inputs(\"text\") for example in dataset.dev]\n",
    "unique_train_labels = {example.label for example in dataset.train}\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print(f\"Train labels: {unique_train_labels}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a683b83-6acc-4fdf-846f-bd81cadda53b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set up DSPy Signature and Module\n",
    "\n",
    "Finally, we will define our task: text classification.\n",
    "\n",
    "There are a variety of ways you can provide guidelines to DSPy signature behavior. Currently, DSPy allows users to specify:\n",
    "\n",
    "1. A high-level goal via the class docstring.\n",
    "2. A set of input fields, with optional metadata.\n",
    "3. A set of output fields with optional metadata.\n",
    "\n",
    "DSPy will then leverage this information to inform optimization. \n",
    "\n",
    "In the below example, note that we simply provide the expected labels to `output` field in the `TextClassificationSignature` class. From this initial state, we'll look to use DSPy to learn to improve our classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0743c63-c679-4114-b7b1-bed4aeb918cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TextClassificationSignature(dspy.Signature):\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(\n",
    "        desc=f\"Label of predicted class. Possible labels are {unique_train_labels}\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TextClassifier(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_classification = dspy.Predict(TextClassificationSignature)\n",
    "\n",
    "    def forward(self, text: str):\n",
    "        return self.generate_classification(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d93df5c-25d7-460d-a803-0e5469f3bbad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89741fa-2698-4409-b290-f5f5652f7d66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Hello World\n",
    "Let's demonstrate predicting via the DSPy module and associated signature. The program has correctly learned our labels from the signature `desc` field and generates reasonable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4da1ad9-7173-4d1b-ab53-346e9c49fbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    label='interest'\n",
      ")\n",
      "Prediction(\n",
      "    label='interest'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initilize our impact_improvement class\n",
    "text_classifier = TextClassifier()\n",
    "\n",
    "message = \"I am interested in space\"\n",
    "print(text_classifier(text=message))\n",
    "\n",
    "message = \"I enjoy ice skating\"\n",
    "print(text_classifier(text=message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Traces\n",
    "\n",
    "1. Open the MLflow UI and select the `\"DSPy Quickstart\"` experiment.\n",
    "2. Go to the `\"Traces\"` tab to view the generated traces.\n",
    "\n",
    "Now, you can observe how DSPy translates your query and interacts with the LLM. This feature is extremely valuable for debugging, iteratively refining components within your system, and monitoring models in production. While the module in this tutorial is relatively simple, the tracing feature becomes even more powerful as your model grows in complexity.\n",
    "\n",
    "![MLflow DSPy Trace](/images/llms/dspy/dspy-trace.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b064a99e-027a-4854-873b-5347d901de46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Compilation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a82d466-40f7-411a-a029-dcadb4629391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "To train, we will leverage [SIMBA](https://dspy.ai/api/optimizers/SIMBA/), an optimizer that will take bootstrap samples from our training set and leverage a random search strategy to optimize our predictive accuracy.\n",
    "\n",
    "Note that in the below example, we leverage a simple metric definition of exact match, as defined in `validate_classification`, but [dspy.Metrics](https://dspy.ai/learn/evaluation/metrics/) can contain complex and LM-based logic to properly evaluate our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d41f13-ef79-45bf-90fa-2abee39c35ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dspy import SIMBA\n",
    "\n",
    "\n",
    "def validate_classification(example, prediction, trace=None) -> bool:\n",
    "    return example.label == prediction.label\n",
    "\n",
    "\n",
    "optimizer = SIMBA(\n",
    "    metric=validate_classification,\n",
    "    max_demos=2,\n",
    "    bsize=12,\n",
    "    num_threads=1,\n",
    ")\n",
    "\n",
    "compiled_pe = optimizer.compile(TextClassifier(), trainset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f09466f-b929-4ebf-adfd-01c9387d3c8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Compare Pre/Post Compiled Accuracy\n",
    "\n",
    "Finally, let's explore how well our trained model can predict on unseen test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c075af7e-b15d-4adb-ab2a-e65bbdc38069",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled accuracy: 0.875\n",
      "Compiled accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(classifier, test_data: pd.DataFrame = test_dataset) -> float:\n",
    "    residuals = []\n",
    "    predictions = []\n",
    "    for example in test_data:\n",
    "        prediction = classifier(text=example[\"text\"])\n",
    "        residuals.append(int(validate_classification(example, prediction)))\n",
    "        predictions.append(prediction)\n",
    "    return residuals, predictions\n",
    "\n",
    "\n",
    "uncompiled_residuals, uncompiled_predictions = check_accuracy(TextClassifier())\n",
    "print(f\"Uncompiled accuracy: {np.mean(uncompiled_residuals)}\")\n",
    "\n",
    "compiled_residuals, compiled_predictions = check_accuracy(compiled_pe)\n",
    "print(f\"Compiled accuracy: {np.mean(compiled_residuals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0964ae5f-6dfe-4b52-8a91-b91d45f69b82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As shown above, our compiled accuracy is non-zero - our base LLM inferred meaning of the classification labels simply via our initial prompt. However, with DSPy training, the prompts, demonstrations, and input/output signatures have been updated to give our model to 100% accuracy on unseen data. That's a gain of 12 percentage points!\n",
    "\n",
    "Let's take a look at each prediction in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9228b66-864b-49fb-838d-33c12e2ff8b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect prediction:    money-fx\n",
      "Correct prediction:      crude\n",
      "Correct prediction:      money-fx\n",
      "Correct prediction:      earn\n",
      "Incorrect prediction:    interest\n",
      "Correct prediction:      grain\n",
      "Correct prediction:      trade\n",
      "Incorrect prediction:    trade\n"
     ]
    }
   ],
   "source": [
    "for uncompiled_residual, uncompiled_prediction in zip(uncompiled_residuals, uncompiled_predictions):\n",
    "    is_correct = \"Correct\" if bool(uncompiled_residual) else \"Incorrect\"\n",
    "    prediction = uncompiled_prediction.label\n",
    "    print(f\"{is_correct} prediction: {' ' * (12 - len(is_correct))}{prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93c1b68-a95e-436e-9c0d-54b2b3e34f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction:      interest\n",
      "Correct prediction:      crude\n",
      "Correct prediction:      money-fx\n",
      "Correct prediction:      earn\n",
      "Correct prediction:      acq\n",
      "Correct prediction:      grain\n",
      "Correct prediction:      trade\n",
      "Correct prediction:      ship\n"
     ]
    }
   ],
   "source": [
    "for compiled_residual, compiled_prediction in zip(compiled_residuals, compiled_predictions):\n",
    "    is_correct = \"Correct\" if bool(compiled_residual) else \"Incorrect\"\n",
    "    prediction = compiled_prediction.label\n",
    "    print(f\"{is_correct} prediction: {' ' * (12 - len(is_correct))}{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73351663-ed84-4e29-8bf5-61fbf94da937",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Log and Load the Model with MLflow\n",
    "\n",
    "Now that we have a compiled model with higher classification accuracy, let's leverage MLflow to log this model and load it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94969489f0444d3a919ccf11f0bc4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.dspy.log_model(\n",
    "        compiled_pe,\n",
    "        name=\"model\",\n",
    "        input_example=\"what is 2 + 2?\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the MLflow UI again and check the complied model is recorded to a new MLflow Run. Now you can load the model back for inference using `mlflow.dspy.load_model` or `mlflow.pyfunc.load_model`.\n",
    "\n",
    "💡 MLflow will remember the environment configuration stored in `dspy.settings`, such as the language model (LM) used during the experiment. This ensures excellent reproducibility for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============Input Text============\n",
      "Text: top discount rate at u k bill tender rises to pct\n",
      "\n",
      "--------------Original DSPy Prediction------------\n",
      "interest\n",
      "\n",
      "--------------Loaded DSPy Prediction------------\n",
      "interest\n",
      "\n",
      "--------------PyFunc Prediction------------\n",
      "interest\n"
     ]
    }
   ],
   "source": [
    "# Define input text\n",
    "print(\"\\n==============Input Text============\")\n",
    "text = test_dataset[0][\"text\"]\n",
    "print(f\"Text: {text}\")\n",
    "\n",
    "# Inference with original DSPy object\n",
    "print(\"\\n--------------Original DSPy Prediction------------\")\n",
    "print(compiled_pe(text=text).label)\n",
    "\n",
    "# Inference with loaded DSPy object\n",
    "print(\"\\n--------------Loaded DSPy Prediction------------\")\n",
    "loaded_model_dspy = mlflow.dspy.load_model(model_info.model_uri)\n",
    "print(loaded_model_dspy(text=text).label)\n",
    "\n",
    "# Inference with MLflow PyFunc API\n",
    "loaded_model_pyfunc = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "print(\"\\n--------------PyFunc Prediction------------\")\n",
    "print(loaded_model_pyfunc.predict(text)[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09199461-878a-4b9f-9eb0-8803775a6cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "This example demonstrates how DSPy works. Below are some potential extensions for improving this project, both with DSPy and MLflow.\n",
    "\n",
    "### DSPy\n",
    "* Use real-world data for the classifier.\n",
    "* Experiment with different optimizers.\n",
    "* For more in-depth examples, check out the [tutorials](https://dspy.ai/tutorials/) and [documentation](https://dspy.ai/learn/).\n",
    "\n",
    "### MLflow\n",
    "* Deploy the model using MLflow serving.\n",
    "* Use MLflow to experiment with various optimization strategies.\n",
    "* Track your DSPy experiments using [DSPy Optimizer Autologging](https://mlflow.org/docs/latest/genai/flavors/dspy/optimizer/).\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Simple DSPy Classifier OSS",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
