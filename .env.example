# ANEETAA Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM and Embedding Model Configuration
# =============================================================================

# Main LLM for agents (via Ollama)
LLM_MODEL=phi4-mini

# Creative LLM for quiz generation (higher temperature)
CREATIVE_LLM_MODEL=phi4-mini

# Embedding model for vector database
EMBEDDING_MODEL=nomic-embed-text

# =============================================================================
# DSPy Configuration
# =============================================================================

# Enable DSPy-optimized agents (true/false)
USE_DSPY_AGENTS=false

# DSPy Language Model (for optimization and inference)
# Options:
#   - openai/gpt-4o-mini (recommended for optimization)
#   - openai/gpt-4
#   - ollama/llama3.1:8b (local option)
DSPY_LM_MODEL=openai/gpt-4o-mini

# OpenAI API Key (required if using OpenAI models)
OPENAI_API_KEY=your_openai_api_key_here

# DSPy cache directory (optional)
DSPY_CACHE_DIR=.dspy_cache

# =============================================================================
# MLflow Configuration
# =============================================================================

# MLflow tracking server URI
# Options:
#  - Local HTTP server: http://<host>:<port>
#  - File store (fast local runs): file://<path-to-mlruns> (default used by notebook when unset)
#  - Databricks: set to the literal value `databricks` and provide DATABRICKS_HOST/TOKEN below
# Example (local dev): MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_TRACKING_URI=http://localhost:5000

# MLflow experiment name (or set MLFLOW_EXPERIMENT_ID to reuse an existing experiment)
MLFLOW_EXPERIMENT_NAME=aneeta-production
# Alternatively you can set an explicit experiment id if provided by teammates
# MLFLOW_EXPERIMENT_ID=

# Enable MLflow tracing (true/false)
MLFLOW_ENABLE_TRACING=true


# -----------------------------------------------------------------------------
# Databricks (optional)
# If you want to use Databricks as the MLflow backend, set MLFLOW_TRACKING_URI=databricks
# and provide the following credentials. DO NOT commit your real token to source control.
# Example:
#   MLFLOW_TRACKING_URI=databricks
#   DATABRICKS_HOST=https://adb-123456789012345.15.azuredatabricks.net
#   DATABRICKS_TOKEN=dapiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# Optionally set MLFLOW_EXPERIMENT_ID to an existing Databricks experiment id.
DATABRICKS_HOST=
DATABRICKS_TOKEN=

# =============================================================================
# Vector Database Configuration
# =============================================================================

# Base path for Chroma vector databases
VECTORDB_BASE_PATH=src/aneeta/vectordb

# =============================================================================
# Application Settings
# =============================================================================

# Default explanation language
DEFAULT_LANGUAGE=Tamil

# Available languages (comma-separated)
AVAILABLE_LANGUAGES=Tamil,Hindi,Bengali,Telugu,Marathi

# =============================================================================
# Advanced Settings (Optional)
# =============================================================================

# Maximum tokens for LLM responses
MAX_TOKENS=700

# LLM temperature (0-1, lower = more deterministic)
LLM_TEMPERATURE=0

# Number of documents to retrieve for RAG
RAG_TOP_K=3

# Enable debug logging (true/false)
DEBUG_MODE=false

# =============================================================================
# Notes
# =============================================================================

# 1. To use DSPy with Ollama (local):
#    - Set DSPY_LM_MODEL=ollama/llama3.1:8b
#    - Ensure Ollama is running: ollama serve
#    - Pull model: ollama pull llama3.1:8b

# 2. To use DSPy with OpenAI:
#    - Set DSPY_LM_MODEL=openai/gpt-4o-mini
#    - Add your OPENAI_API_KEY above

# 3. To start MLflow server:
#    - Run: mlflow ui --port 5000
#    - Access: http://localhost:5000

# 4. To enable DSPy agents:
#    - Set USE_DSPY_AGENTS=true
#    - Ensure you have optimized models in MLflow or use unoptimized versions

# 5. For production deployment:
#    - Use optimized DSPy models from MLflow Model Registry
#    - Enable MLflow tracing for monitoring
#    - Set appropriate resource limits
